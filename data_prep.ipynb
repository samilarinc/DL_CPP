{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('other/train.csv')\n",
    "y = df[\"Habitability_score\"].values\n",
    "ids = df['Property_ID'].values  \n",
    "df.drop(['Habitability_score', 'Property_ID'], axis=1, inplace=True)\n",
    "n_data = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DataPrep(object):\n",
    "    def __init__(self, df, y):\n",
    "        self.df = df.copy()\n",
    "        self.y = y.copy()\n",
    "        self.n_data = df.shape[0]\n",
    "        self.arrs = []\n",
    "        self.cols = []\n",
    "        self.nans = []\n",
    "    \n",
    "    def search_nan(self):\n",
    "        for i in self.df:\n",
    "            for j in self.df[i]:\n",
    "                if j != j:\n",
    "                    self.nans.append(i)\n",
    "                    break\n",
    "\n",
    "    def train_test_split(self, test_size=0.15, val_size=0.15):\n",
    "        if self.prepared_df is not None:\n",
    "            n_train = int(self.n_data*(1 - test_size - val_size))\n",
    "            n_test = int(self.n_data*(test_size))\n",
    "            n_val = self.n_data - n_train - n_test\n",
    "            arr_data = self.prepared_df.to_numpy()\n",
    "            arr_y = self.y\n",
    "            np.random.seed(31)\n",
    "            p = np.random.permutation(len(self.y))\n",
    "            arr_data = arr_data[p]\n",
    "            arr_y = arr_y[p]\n",
    "            if val_size == 0:\n",
    "                return arr_data[:n_train], arr_y[:n_train], arr_data[n_train:], arr_y[n_train:]\n",
    "            else:\n",
    "                return arr_data[:n_train], arr_y[:n_train], arr_data[n_train:n_train+n_val], arr_y[n_train:n_train+n_val], arr_data[n_train+n_val:], arr_y[n_train+n_val:]\n",
    "        else:\n",
    "            print(\"Run prepare() first!!!\")\n",
    "        \n",
    "    def fill_categorical_nan_and_one_hot_encode(self, col):\n",
    "        arr = self.df[col].to_numpy()\n",
    "        dct = dict()\n",
    "        nan_ind = []\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            element = arr[i]\n",
    "            if isinstance(element, str) and element != 'nan':\n",
    "                try:\n",
    "                    dct[element] += 1\n",
    "                except:\n",
    "                    dct[element] = 1\n",
    "            else:\n",
    "                nan_ind.append(i)\n",
    "\n",
    "        nonnan_len = len(arr) - len(nan_ind)\n",
    "        for i in dct:\n",
    "            dct[i] = dct[i]/nonnan_len\n",
    "        for i in nan_ind:\n",
    "            # arr[i] = np.random.choice(list(dct.keys()), p=list(dct.values()))\n",
    "            arr[i] = \"nan\"\n",
    "        freq_arr = np.zeros(len(dct.keys()))\n",
    "        uni_vals = np.unique(arr).tolist()\n",
    "        uni_vals.remove(\"nan\")\n",
    "        encoded_arr = np.zeros((self.n_data, len(uni_vals)))\n",
    "        encod_dict = dict()\n",
    "        for i in range(len(uni_vals)):\n",
    "            encod_dict[uni_vals[i]] = i\n",
    "        for i in range(len(freq_arr)):\n",
    "            freq_arr[i] = dct[uni_vals[i]]\n",
    "        for i in range(self.n_data):\n",
    "            if arr[i] == \"nan\":\n",
    "                encoded_arr[i] = freq_arr.copy()\n",
    "            else:\n",
    "                encoded_arr[i][encod_dict[arr[i]]] = 1\n",
    "        return encoded_arr, encod_dict\n",
    "\n",
    "    def fill_numerical_nan(self, col):\n",
    "        return self.df[col].fillna(self.df[col].mean())\n",
    "\n",
    "    def one_hot_encode_not_nan(self, col):\n",
    "        arr = self.df[col].to_numpy()\n",
    "        dct = dict()\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            element = arr[i]\n",
    "            try:\n",
    "                dct[element] += 1\n",
    "            except:\n",
    "                dct[element] = 1\n",
    "\n",
    "        uni_vals = np.unique(arr).tolist()\n",
    "        encoded_arr = np.zeros((self.n_data, len(uni_vals)))\n",
    "        encod_dict = dict()\n",
    "        for i in range(len(uni_vals)):\n",
    "            encod_dict[uni_vals[i]] = i\n",
    "        for i in range(self.n_data):\n",
    "            encoded_arr[i][encod_dict[arr[i]]] = 1\n",
    "        return encoded_arr, encod_dict\n",
    "\n",
    "    def normalize_numerical(self, arr):\n",
    "        return (arr - arr.mean())/arr.std()\n",
    "\n",
    "    def prepare(self):\n",
    "        d_types = self.df.dtypes\n",
    "        self.search_nan()\n",
    "        for i in self.df:\n",
    "            if i in self.nans:\n",
    "                if d_types[i] == \"object\":\n",
    "                    ret_tuple = self.fill_categorical_nan_and_one_hot_encode(i)\n",
    "                    for k in ret_tuple[0].T:\n",
    "                        self.arrs.append(k)\n",
    "                    for j in ret_tuple[1]:\n",
    "                        self.cols.append(i+'_'+j)\n",
    "                else:\n",
    "                    self.arrs.append(self.normalize_numerical(self.fill_numerical_nan(i)))\n",
    "                    self.cols.append(i)\n",
    "            else:\n",
    "                if d_types[i] == \"object\":\n",
    "                    ret_tuple = self.one_hot_encode_not_nan(i)\n",
    "                    for k in ret_tuple[0].T:\n",
    "                        self.arrs.append(k)\n",
    "                    for j in ret_tuple[1]:\n",
    "                        self.cols.append(i+'_'+j)\n",
    "                else:\n",
    "                    self.arrs.append(self.normalize_numerical(self.df[i]))\n",
    "                    self.cols.append(i)\n",
    "        self.prepared_df = pd.DataFrame(np.array(self.arrs).T, columns=self.cols)\n",
    "        return pd.DataFrame(np.array(self.arrs).T, columns=self.cols), self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(net, X, y):\n",
    "    y_hat = net.test(X)\n",
    "    y_hat = np.array(y_hat.tolist())\n",
    "    y_hat = y_hat.squeeze()\n",
    "    y = np.array(y.tolist())\n",
    "    y = y.squeeze()\n",
    "    return np.mean((y_hat - y)**2)\n",
    "\n",
    "def plot(loss, valloss):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(loss, label = 'train loss')\n",
    "    plt.plot(valloss, label = 'validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(31599, 31), (7900, 31)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_set = DataPrep(df, y)\n",
    "# df_new, labels = d_set.prepare()\n",
    "# X_train, y_train, X_val, y_val = d_set.train_test_split()\n",
    "# \"(%d, %d), (%d, %d)\" % (X_train.shape[0], X_train.shape[1], X_val.shape[0], X_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39499 data available with 31 features (some are one hot encoded)\n",
      "27649, 5926, 5924\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('other/train.csv')\n",
    "y = df[\"Habitability_score\"].values\n",
    "ids = df['Property_ID'].values  \n",
    "df.drop(['Habitability_score', 'Property_ID'], axis=1, inplace=True)\n",
    "preparator = DataPrep(df, y)\n",
    "PreparedData, labels = preparator.prepare()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preparator.train_test_split(0.15, 0.15)\n",
    "n_data, n_features = PreparedData.shape[0:2]\n",
    "X = X_train\n",
    "y = y_train\n",
    "print(\"%d data available with %d features (some are one hot encoded)\"%(n_data, n_features))\n",
    "print(\"%d, %d, %d\"%(X_train.shape[0], X_val.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39499 data available with 31 features (some are one hot encoded)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = PreparedData.to_numpy()\n",
    "y = labels\n",
    "n_features = X.shape[1]\n",
    "\"%d data available with %d features (some are one hot encoded)\"%(n_data, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "np.save(path + 'X_train.npy', X_train)\n",
    "np.save(path + 'y_train.npy', y_train)\n",
    "np.save(path + 'X_val.npy', X_val)\n",
    "np.save(path + 'y_val.npy', y_val)\n",
    "np.save(path + 'X_test.npy', X_test)\n",
    "np.save(path + 'y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1\n",
      "\tTrain Loss: 5080.08\tVal Loss: 4666.12\n",
      "Epoch:    2\n",
      "\tTrain Loss: 4286.95\tVal Loss: 3921.30\n",
      "Epoch:    3\n",
      "\tTrain Loss: 3589.15\tVal Loss: 3272.18\n",
      "Epoch:    4\n",
      "\tTrain Loss: 2984.94\tVal Loss: 2713.58\n",
      "Epoch:    5\n",
      "\tTrain Loss: 2467.51\tVal Loss: 2237.59\n",
      "Epoch:    6\n",
      "\tTrain Loss: 2028.65\tVal Loss: 1835.87\n",
      "Epoch:    7\n",
      "\tTrain Loss: 1660.10\tVal Loss: 1500.25\n",
      "Epoch:    8\n",
      "\tTrain Loss: 1353.78\tVal Loss: 1222.81\n",
      "Epoch:    9\n",
      "\tTrain Loss: 1101.95\tVal Loss: 996.00\n",
      "Epoch:   10\n",
      "\tTrain Loss: 897.27\tVal Loss: 812.73\n",
      "Epoch:   11\n",
      "\tTrain Loss: 732.88\tVal Loss: 666.42\n",
      "Epoch:   12\n",
      "\tTrain Loss: 602.47\tVal Loss: 551.05\n",
      "Epoch:   13\n",
      "\tTrain Loss: 500.32\tVal Loss: 461.24\n",
      "Epoch:   14\n",
      "\tTrain Loss: 421.35\tVal Loss: 392.23\n",
      "Epoch:   15\n",
      "\tTrain Loss: 361.10\tVal Loss: 339.89\n",
      "Epoch:   16\n",
      "\tTrain Loss: 315.76\tVal Loss: 300.73\n",
      "Epoch:   17\n",
      "\tTrain Loss: 282.10\tVal Loss: 271.82\n",
      "Epoch:   18\n",
      "\tTrain Loss: 257.45\tVal Loss: 250.76\n",
      "Epoch:   19\n",
      "\tTrain Loss: 239.66\tVal Loss: 235.62\n",
      "Epoch:   20\n",
      "\tTrain Loss: 226.99\tVal Loss: 224.89\n",
      "Epoch:   21\n",
      "\tTrain Loss: 218.09\tVal Loss: 217.37\n",
      "Epoch:   22\n",
      "\tTrain Loss: 211.94\tVal Loss: 212.18\n",
      "Epoch:   23\n",
      "\tTrain Loss: 207.73\tVal Loss: 208.63\n",
      "Epoch:   24\n",
      "\tTrain Loss: 204.90\tVal Loss: 206.24\n",
      "Epoch:   25\n",
      "\tTrain Loss: 203.02\tVal Loss: 204.64\n",
      "Epoch:   26\n",
      "\tTrain Loss: 201.79\tVal Loss: 203.59\n",
      "Epoch:   27\n",
      "\tTrain Loss: 201.00\tVal Loss: 202.90\n",
      "Epoch:   28\n",
      "\tTrain Loss: 200.50\tVal Loss: 202.45\n",
      "Epoch:   29\n",
      "\tTrain Loss: 200.18\tVal Loss: 202.17\n",
      "Epoch:   30\n",
      "\tTrain Loss: 199.98\tVal Loss: 201.98\n",
      "Epoch:   31\n",
      "\tTrain Loss: 199.86\tVal Loss: 201.86\n",
      "Epoch:   32\n",
      "\tTrain Loss: 199.79\tVal Loss: 201.79\n",
      "Epoch:   33\n",
      "\tTrain Loss: 199.75\tVal Loss: 201.74\n",
      "Epoch:   34\n",
      "\tTrain Loss: 199.72\tVal Loss: 201.71\n",
      "Epoch:   35\n",
      "\tTrain Loss: 199.71\tVal Loss: 201.69\n",
      "Epoch:   36\n",
      "\tTrain Loss: 199.70\tVal Loss: 201.68\n",
      "Epoch:   37\n",
      "\tTrain Loss: 199.70\tVal Loss: 201.67\n",
      "Epoch:   38\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.66\n",
      "Epoch:   39\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.66\n",
      "Epoch:   40\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.66\n",
      "Epoch:   41\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   42\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   43\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   44\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   45\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   46\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   47\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   48\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   49\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   50\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   51\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   52\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   53\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   54\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   55\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   56\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   57\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   58\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   59\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   60\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   61\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   62\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   63\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   64\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   65\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   66\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   67\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   68\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   69\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   70\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   71\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   72\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   73\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   74\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   75\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   76\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   77\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   78\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   79\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   80\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   81\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   82\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   83\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   84\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   85\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   86\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   87\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   88\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   89\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   90\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   91\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   92\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   93\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   94\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   95\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   96\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   97\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   98\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   99\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  100\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  101\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  102\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  103\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  104\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  105\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  106\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  107\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  108\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  109\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  110\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  111\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  112\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  113\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  114\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  115\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  116\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  117\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  118\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  119\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  120\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  121\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  122\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  123\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  124\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  125\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  126\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  127\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  128\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  129\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  130\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  131\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  132\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  133\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  134\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  135\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  136\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  137\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  138\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  139\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  140\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  141\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  142\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  143\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  144\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  145\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  146\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  147\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  148\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  149\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  150\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  151\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  152\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  153\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  154\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  155\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  156\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  157\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  158\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  159\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  160\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  161\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  162\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  163\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  164\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  165\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  166\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  167\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  168\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  169\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  170\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  171\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  172\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  173\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  174\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  175\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  176\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  177\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  178\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  179\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  180\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  181\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  182\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  183\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  184\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  185\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  186\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  187\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  188\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  189\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  190\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  191\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  192\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  193\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  194\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  195\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  196\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  197\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  198\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  199\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  200\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  201\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  202\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  203\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  204\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  205\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  206\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  207\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  208\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  209\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  210\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  211\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  212\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  213\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  214\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  215\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  216\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  217\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  218\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  219\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  220\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  221\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  222\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  223\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  224\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  225\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  226\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  227\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  228\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  229\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  230\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  231\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  232\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  233\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  234\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  235\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  236\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  237\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  238\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  239\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  240\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  241\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  242\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  243\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  244\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  245\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  246\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  247\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  248\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  249\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  250\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/loss_with_(16, 4, 2) 2048, 0.1, 0.8, Adam, Sigmoid, L2Loss, 250, 199.69340270145713, 201.6522922608246.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m collect()\n\u001b[1;32m     39\u001b[0m name_str \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhidden_sizes\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmomentum\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mOptimizer\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmy_activations[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mloss_layer\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mloss[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalloss[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 40\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mruns/loss_with_\u001b[39;49m\u001b[39m{\u001b[39;49;00mname_str\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, loss)\n\u001b[1;32m     41\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mruns/valloss_with_\u001b[39m\u001b[39m{\u001b[39;00mname_str\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m, valloss)\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mruns/train_hist.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:518\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    517\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/loss_with_(16, 4, 2) 2048, 0.1, 0.8, Adam, Sigmoid, L2Loss, 250, 199.69340270145713, 201.6522922608246.npy'"
     ]
    }
   ],
   "source": [
    "from NeuralNetwork import NeuralNetwork as nn\n",
    "from data_layer import Dataset\n",
    "from pyflow import Tensor, L1Loss, L2Loss, FullyConnected, SGD, CrossEntropyLoss, ReLU, Sigmoid\n",
    "from gc import collect\n",
    "collect()\n",
    "\n",
    "y_val = Tensor(y_val[:, None, None]) if type(y_val) == np.ndarray else y_val\n",
    "batch_size = 2048\n",
    "dataset = Dataset(X, y, batch_size, True)\n",
    "loss_layer = L2Loss()\n",
    "net = nn(dataset, loss_layer)\n",
    "\n",
    "in_size = n_features\n",
    "hidden_sizes = (16, 4, 2)\n",
    "out_size = 1\n",
    "\n",
    "momentum = 0.8\n",
    "lr = 1e-1\n",
    "Optimizer = 'Adam'\n",
    "\n",
    "my_layers = []\n",
    "my_activations = []\n",
    "my_layers.append(FullyConnected(in_size, hidden_sizes[0], lr, Optimizer, 0.9, 0.999))\n",
    "my_layers.append(FullyConnected(hidden_sizes[0], hidden_sizes[1], lr, Optimizer, 0.9, 0.999))\n",
    "my_layers.append(FullyConnected(hidden_sizes[1], hidden_sizes[2], lr, Optimizer, 0.9, 0.999))\n",
    "my_layers.append(FullyConnected(hidden_sizes[2], out_size, lr, Optimizer, 0.9, 0.999))\n",
    "\n",
    "my_activations.append(Sigmoid())\n",
    "my_activations.append(Sigmoid())\n",
    "my_activations.append(Sigmoid())\n",
    "\n",
    "for i in range(len(my_layers)):\n",
    "    net.append_layer(my_layers[i])\n",
    "    if i < len(my_activations):\n",
    "        net.append_layer(my_activations[i])\n",
    "epochs = 250\n",
    "loss, valloss = net.train(epochs, cross_val = True, valset = (X_val, y_val), verbose = True)\n",
    "collect()\n",
    "name_str = f'{hidden_sizes} {batch_size}, {lr}, {momentum}, {Optimizer}, {my_activations[0].__class__.__name__}, {loss_layer.__class__.__name__}, {epochs}, {loss[-1]}, {valloss[-1]}'\n",
    "np.save(f'runs/loss_with_{name_str}.npy', loss)\n",
    "np.save(f'runs/valloss_with_{name_str}.npy', valloss)\n",
    "with open('runs/train_hist.txt', 'a') as f:\n",
    "    print(name_str, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss(net, X_val, y_val), calculate_loss(net, X, y)\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.arange(0, epochs)\n",
    "loss = net.loss\n",
    "valloss = net.vallosses\n",
    "plt.plot(t, loss, label = 'train loss')\n",
    "plt.plot(t, valloss, label = 'validation loss')\n",
    "plt.legend()\n",
    "print(valloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_model('model_Adam_48_valllos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "for i in listdir('runs'):\n",
    "    if i.startswith('loss_with_'):\n",
    "        name = i[10:]\n",
    "        train_loss = np.load(f'runs/{i}', allow_pickle=True)\n",
    "        val_loss = np.load(f'runs/valloss_with_{name}', allow_pickle=True)\n",
    "        labels = name[:-4]\n",
    "        plot(train_loss, val_loss)\n",
    "        splitted = labels.split(', ')\n",
    "        try:\n",
    "            splitted[-2] = float(splitted[-2])\n",
    "            splitted[-1] = float(splitted[-1])\n",
    "            print(\", \".join(splitted[:-2]), \"%.2f\"%splitted[-2], \"%.2f\"%splitted[-1])\n",
    "        except:\n",
    "            print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import NeuralNetwork as nn\n",
    "from data_layer import Dataset\n",
    "from pyflow import Tensor, L1Loss, L2Loss, FullyConnected, SGD, CrossEntropyLoss, ReLU\n",
    "from gc import collect\n",
    "import numpy as np\n",
    "collect()\n",
    "\n",
    "layer = FullyConnected(31, 16, 1e-3, 'SGD')\n",
    "layer2 = FullyConnected(16, 8, 1e-3, 'SGD')\n",
    "layer3 = FullyConnected(8, 4, 1e-3, 'SGD')\n",
    "layer4 = FullyConnected(4, 1, 1e-3, 'SGD')\n",
    "\n",
    "net = nn('SGD', path='saved_models/model_Adam_48_valllos')\n",
    "net.append_layer(layer)\n",
    "net.append_layer(ReLU())\n",
    "net.append_layer(layer2)\n",
    "net.append_layer(ReLU())\n",
    "net.append_layer(layer3)\n",
    "net.append_layer(ReLU())\n",
    "net.append_layer(layer4)\n",
    "\n",
    "net.loss_layer = L1Loss()\n",
    "\n",
    "net.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn('SGD')\n",
    "net2.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFullyConnected\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m     W1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     W2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net2\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     b1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     b2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net2\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net2' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(net.layers)):\n",
    "    if net.layers[i].__class__.__name__ == 'FullyConnected':\n",
    "        W1 = np.array(net.layers[i].weights.tolist())[0]\n",
    "        W2 = np.array(net2.layers[i].weights.tolist())[0]\n",
    "        b1 = np.array(net.layers[i].bias.tolist())[0]\n",
    "        b2 = np.array(net2.layers[i].bias.tolist())[0]\n",
    "        print(np.allclose(W1, W2))\n",
    "        print(np.allclose(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.592845 0.844266 0.857946 0.847252 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pkl.dump(df_new.to_numpy(), f)\n",
    "\n",
    "with open('labels.pkl', 'wb') as f:\n",
    "    pkl.dump(labels, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
