{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('other/train.csv')\n",
    "y = df[\"Habitability_score\"].values\n",
    "ids = df['Property_ID'].values  \n",
    "df.drop(['Habitability_score', 'Property_ID'], axis=1, inplace=True)\n",
    "n_data = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep(object):\n",
    "    def __init__(self, df, y):\n",
    "        self.df = df.copy()\n",
    "        self.y = y.copy()\n",
    "        self.n_data = df.shape[0]\n",
    "        self.arrs = []\n",
    "        self.cols = []\n",
    "        self.nans = []\n",
    "    \n",
    "    def search_nan(self):\n",
    "        for i in self.df:\n",
    "            for j in self.df[i]:\n",
    "                if j != j:\n",
    "                    self.nans.append(i)\n",
    "                    break\n",
    "\n",
    "    def train_test_split(self, test_size=0.2):\n",
    "        if self.prepared_df is not None:\n",
    "            n_train = int(self.n_data*(1-test_size))\n",
    "            arr_data = self.prepared_df.to_numpy()\n",
    "            arr_y = self.y\n",
    "            np.random.seed(31)\n",
    "            p = np.random.permutation(len(self.y))\n",
    "            arr_data = arr_data[p]\n",
    "            arr_y = arr_y[p]\n",
    "            return arr_data[:n_train], arr_y[:n_train], arr_data[n_train:], arr_y[n_train:]\n",
    "        else:\n",
    "            print(\"Run prepare() first!!!\")\n",
    "        \n",
    "    def fill_categorical_nan_and_one_hot_encode(self, col):\n",
    "        arr = self.df[col].to_numpy()\n",
    "        dct = dict()\n",
    "        nan_ind = []\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            element = arr[i]\n",
    "            if isinstance(element, str) and element != 'nan':\n",
    "                try:\n",
    "                    dct[element] += 1\n",
    "                except:\n",
    "                    dct[element] = 1\n",
    "            else:\n",
    "                nan_ind.append(i)\n",
    "\n",
    "        nonnan_len = len(arr) - len(nan_ind)\n",
    "        for i in dct:\n",
    "            dct[i] = dct[i]/nonnan_len\n",
    "        for i in nan_ind:\n",
    "            # arr[i] = np.random.choice(list(dct.keys()), p=list(dct.values()))\n",
    "            arr[i] = \"nan\"\n",
    "        freq_arr = np.zeros(len(dct.keys()))\n",
    "        uni_vals = np.unique(arr).tolist()\n",
    "        uni_vals.remove(\"nan\")\n",
    "        encoded_arr = np.zeros((self.n_data, len(uni_vals)))\n",
    "        encod_dict = dict()\n",
    "        for i in range(len(uni_vals)):\n",
    "            encod_dict[uni_vals[i]] = i\n",
    "        for i in range(len(freq_arr)):\n",
    "            freq_arr[i] = dct[uni_vals[i]]\n",
    "        for i in range(self.n_data):\n",
    "            if arr[i] == \"nan\":\n",
    "                encoded_arr[i] = freq_arr.copy()\n",
    "            else:\n",
    "                encoded_arr[i][encod_dict[arr[i]]] = 1\n",
    "        return encoded_arr, encod_dict\n",
    "\n",
    "    def fill_numerical_nan(self, col):\n",
    "        return self.df[col].fillna(self.df[col].mean())\n",
    "\n",
    "    def one_hot_encode_not_nan(self, col):\n",
    "        arr = self.df[col].to_numpy()\n",
    "        dct = dict()\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            element = arr[i]\n",
    "            try:\n",
    "                dct[element] += 1\n",
    "            except:\n",
    "                dct[element] = 1\n",
    "\n",
    "        uni_vals = np.unique(arr).tolist()\n",
    "        encoded_arr = np.zeros((self.n_data, len(uni_vals)))\n",
    "        encod_dict = dict()\n",
    "        for i in range(len(uni_vals)):\n",
    "            encod_dict[uni_vals[i]] = i\n",
    "        for i in range(self.n_data):\n",
    "            encoded_arr[i][encod_dict[arr[i]]] = 1\n",
    "        return encoded_arr, encod_dict\n",
    "\n",
    "    def normalize_numerical(self, arr):\n",
    "        return (arr - arr.mean())/arr.std()\n",
    "\n",
    "    def prepare(self):\n",
    "        d_types = self.df.dtypes\n",
    "        self.search_nan()\n",
    "        for i in self.df:\n",
    "            if i in self.nans:\n",
    "                if d_types[i] == \"object\":\n",
    "                    ret_tuple = self.fill_categorical_nan_and_one_hot_encode(i)\n",
    "                    for k in ret_tuple[0].T:\n",
    "                        self.arrs.append(k)\n",
    "                    for j in ret_tuple[1]:\n",
    "                        self.cols.append(i+'_'+j)\n",
    "                else:\n",
    "                    self.arrs.append(self.normalize_numerical(self.fill_numerical_nan(i)))\n",
    "                    self.cols.append(i)\n",
    "            else:\n",
    "                if d_types[i] == \"object\":\n",
    "                    ret_tuple = self.one_hot_encode_not_nan(i)\n",
    "                    for k in ret_tuple[0].T:\n",
    "                        self.arrs.append(k)\n",
    "                    for j in ret_tuple[1]:\n",
    "                        self.cols.append(i+'_'+j)\n",
    "                else:\n",
    "                    self.arrs.append(self.normalize_numerical(self.df[i]))\n",
    "                    self.cols.append(i)\n",
    "        self.prepared_df = pd.DataFrame(np.array(self.arrs).T, columns=self.cols)\n",
    "        return pd.DataFrame(np.array(self.arrs).T, columns=self.cols), self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(net, X, y):\n",
    "    y_hat = net.test(X)\n",
    "    y_hat = np.array(y_hat.tolist())\n",
    "    y_hat = y_hat.squeeze()\n",
    "    y = np.array(y.tolist())\n",
    "    y = y.squeeze()\n",
    "    return np.mean((y_hat - y)**2)\n",
    "\n",
    "def plot(loss, valloss):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(loss, label = 'train loss')\n",
    "    plt.plot(valloss, label = 'validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(31599, 31), (7900, 31)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_set = DataPrep(df, y)\n",
    "df_new, labels = d_set.prepare()\n",
    "X_train, y_train, X_val, y_val = d_set.train_test_split()\n",
    "\"(%d, %d), (%d, %d)\" % (X_train.shape[0], X_train.shape[1], X_val.shape[0], X_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Property_Type</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Number_of_Windows</th>\n",
       "      <th>Number_of_Doors</th>\n",
       "      <th>Furnishing</th>\n",
       "      <th>Frequency_of_Powercuts</th>\n",
       "      <th>Power_Backup</th>\n",
       "      <th>Water_Supply</th>\n",
       "      <th>Traffic_Density_Score</th>\n",
       "      <th>Crime_Rate</th>\n",
       "      <th>Dust_and_Noise</th>\n",
       "      <th>Air_Quality_Index</th>\n",
       "      <th>Neighborhood_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apartment</td>\n",
       "      <td>106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Semi_Furnished</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Once in a day - Morning</td>\n",
       "      <td>5.89</td>\n",
       "      <td>Slightly below average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apartment</td>\n",
       "      <td>733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Once in a day - Evening</td>\n",
       "      <td>4.37</td>\n",
       "      <td>Well below average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apartment</td>\n",
       "      <td>737</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Once in a day - Morning</td>\n",
       "      <td>7.45</td>\n",
       "      <td>Slightly below average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>121.0</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apartment</td>\n",
       "      <td>900</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Once in a day - Morning</td>\n",
       "      <td>6.16</td>\n",
       "      <td>Well above average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bungalow</td>\n",
       "      <td>2238</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>All time</td>\n",
       "      <td>5.46</td>\n",
       "      <td>Well below average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>116.0</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39494</th>\n",
       "      <td>Single-family home</td>\n",
       "      <td>1120</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>All time</td>\n",
       "      <td>5.55</td>\n",
       "      <td>Slightly above average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39495</th>\n",
       "      <td>Apartment</td>\n",
       "      <td>445</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>All time</td>\n",
       "      <td>5.70</td>\n",
       "      <td>Slightly above average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39496</th>\n",
       "      <td>Bungalow</td>\n",
       "      <td>3780</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Unfurnished</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Once in two days</td>\n",
       "      <td>6.84</td>\n",
       "      <td>Well below average</td>\n",
       "      <td>Medium</td>\n",
       "      <td>137.0</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39497</th>\n",
       "      <td>Single-family home</td>\n",
       "      <td>1266</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Semi_Furnished</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Once in a day - Morning</td>\n",
       "      <td>4.60</td>\n",
       "      <td>Slightly above average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39498</th>\n",
       "      <td>Single-family home</td>\n",
       "      <td>1229</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Fully Furnished</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>All time</td>\n",
       "      <td>8.29</td>\n",
       "      <td>Well below average</td>\n",
       "      <td>High</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39499 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Property_Type  Property_Area  Number_of_Windows  Number_of_Doors  \\\n",
       "0               Apartment            106                NaN                1   \n",
       "1               Apartment            733                2.0                2   \n",
       "2               Apartment            737                4.0                2   \n",
       "3               Apartment            900                3.0                2   \n",
       "4                Bungalow           2238               14.0                6   \n",
       "...                   ...            ...                ...              ...   \n",
       "39494  Single-family home           1120                3.0                2   \n",
       "39495           Apartment            445                1.0                3   \n",
       "39496            Bungalow           3780                6.0                6   \n",
       "39497  Single-family home           1266                3.0                1   \n",
       "39498  Single-family home           1229                2.0                4   \n",
       "\n",
       "            Furnishing  Frequency_of_Powercuts Power_Backup  \\\n",
       "0       Semi_Furnished                     0.0           No   \n",
       "1          Unfurnished                     1.0           No   \n",
       "2      Fully Furnished                     0.0           No   \n",
       "3          Unfurnished                     2.0          Yes   \n",
       "4      Fully Furnished                     0.0           No   \n",
       "...                ...                     ...          ...   \n",
       "39494              NaN                     0.0           No   \n",
       "39495  Fully Furnished                     1.0           No   \n",
       "39496      Unfurnished                     0.0          Yes   \n",
       "39497   Semi_Furnished                     0.0           No   \n",
       "39498  Fully Furnished                     0.0           No   \n",
       "\n",
       "                  Water_Supply  Traffic_Density_Score              Crime_Rate  \\\n",
       "0      Once in a day - Morning                   5.89  Slightly below average   \n",
       "1      Once in a day - Evening                   4.37      Well below average   \n",
       "2      Once in a day - Morning                   7.45  Slightly below average   \n",
       "3      Once in a day - Morning                   6.16      Well above average   \n",
       "4                     All time                   5.46      Well below average   \n",
       "...                        ...                    ...                     ...   \n",
       "39494                 All time                   5.55  Slightly above average   \n",
       "39495                 All time                   5.70  Slightly above average   \n",
       "39496         Once in two days                   6.84      Well below average   \n",
       "39497  Once in a day - Morning                   4.60  Slightly above average   \n",
       "39498                 All time                   8.29      Well below average   \n",
       "\n",
       "      Dust_and_Noise  Air_Quality_Index  Neighborhood_Review  \n",
       "0             Medium               90.0                 3.86  \n",
       "1             Medium               96.0                 3.55  \n",
       "2             Medium              121.0                 3.81  \n",
       "3             Medium              100.0                 1.34  \n",
       "4             Medium              116.0                 4.77  \n",
       "...              ...                ...                  ...  \n",
       "39494         Medium               80.0                 3.56  \n",
       "39495         Medium               86.0                 2.93  \n",
       "39496         Medium              137.0                 3.80  \n",
       "39497            NaN               88.0                 3.25  \n",
       "39498           High              132.0                 4.67  \n",
       "\n",
       "[39499 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39499 data available with 31 features (some are one hot encoded)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_new.to_numpy()\n",
    "y = labels\n",
    "n_features = X.shape[1]\n",
    "\"%d data available with %d features (some are one hot encoded)\"%(n_data, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1\n",
      "\tTrain Loss: 5080.08\tVal Loss: 4666.12\n",
      "Epoch:    2\n",
      "\tTrain Loss: 4286.95\tVal Loss: 3921.30\n",
      "Epoch:    3\n",
      "\tTrain Loss: 3589.15\tVal Loss: 3272.18\n",
      "Epoch:    4\n",
      "\tTrain Loss: 2984.94\tVal Loss: 2713.58\n",
      "Epoch:    5\n",
      "\tTrain Loss: 2467.51\tVal Loss: 2237.59\n",
      "Epoch:    6\n",
      "\tTrain Loss: 2028.65\tVal Loss: 1835.87\n",
      "Epoch:    7\n",
      "\tTrain Loss: 1660.10\tVal Loss: 1500.25\n",
      "Epoch:    8\n",
      "\tTrain Loss: 1353.78\tVal Loss: 1222.81\n",
      "Epoch:    9\n",
      "\tTrain Loss: 1101.95\tVal Loss: 996.00\n",
      "Epoch:   10\n",
      "\tTrain Loss: 897.27\tVal Loss: 812.73\n",
      "Epoch:   11\n",
      "\tTrain Loss: 732.88\tVal Loss: 666.42\n",
      "Epoch:   12\n",
      "\tTrain Loss: 602.47\tVal Loss: 551.05\n",
      "Epoch:   13\n",
      "\tTrain Loss: 500.32\tVal Loss: 461.24\n",
      "Epoch:   14\n",
      "\tTrain Loss: 421.35\tVal Loss: 392.23\n",
      "Epoch:   15\n",
      "\tTrain Loss: 361.10\tVal Loss: 339.89\n",
      "Epoch:   16\n",
      "\tTrain Loss: 315.76\tVal Loss: 300.73\n",
      "Epoch:   17\n",
      "\tTrain Loss: 282.10\tVal Loss: 271.82\n",
      "Epoch:   18\n",
      "\tTrain Loss: 257.45\tVal Loss: 250.76\n",
      "Epoch:   19\n",
      "\tTrain Loss: 239.66\tVal Loss: 235.62\n",
      "Epoch:   20\n",
      "\tTrain Loss: 226.99\tVal Loss: 224.89\n",
      "Epoch:   21\n",
      "\tTrain Loss: 218.09\tVal Loss: 217.37\n",
      "Epoch:   22\n",
      "\tTrain Loss: 211.94\tVal Loss: 212.18\n",
      "Epoch:   23\n",
      "\tTrain Loss: 207.73\tVal Loss: 208.63\n",
      "Epoch:   24\n",
      "\tTrain Loss: 204.90\tVal Loss: 206.24\n",
      "Epoch:   25\n",
      "\tTrain Loss: 203.02\tVal Loss: 204.64\n",
      "Epoch:   26\n",
      "\tTrain Loss: 201.79\tVal Loss: 203.59\n",
      "Epoch:   27\n",
      "\tTrain Loss: 201.00\tVal Loss: 202.90\n",
      "Epoch:   28\n",
      "\tTrain Loss: 200.50\tVal Loss: 202.45\n",
      "Epoch:   29\n",
      "\tTrain Loss: 200.18\tVal Loss: 202.17\n",
      "Epoch:   30\n",
      "\tTrain Loss: 199.98\tVal Loss: 201.98\n",
      "Epoch:   31\n",
      "\tTrain Loss: 199.86\tVal Loss: 201.86\n",
      "Epoch:   32\n",
      "\tTrain Loss: 199.79\tVal Loss: 201.79\n",
      "Epoch:   33\n",
      "\tTrain Loss: 199.75\tVal Loss: 201.74\n",
      "Epoch:   34\n",
      "\tTrain Loss: 199.72\tVal Loss: 201.71\n",
      "Epoch:   35\n",
      "\tTrain Loss: 199.71\tVal Loss: 201.69\n",
      "Epoch:   36\n",
      "\tTrain Loss: 199.70\tVal Loss: 201.68\n",
      "Epoch:   37\n",
      "\tTrain Loss: 199.70\tVal Loss: 201.67\n",
      "Epoch:   38\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.66\n",
      "Epoch:   39\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.66\n",
      "Epoch:   40\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.66\n",
      "Epoch:   41\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   42\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   43\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   44\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   45\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   46\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   47\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   48\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   49\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   50\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   51\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   52\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   53\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   54\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   55\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   56\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   57\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   58\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   59\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   60\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   61\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   62\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   63\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   64\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   65\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   66\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   67\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   68\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   69\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   70\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   71\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   72\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   73\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   74\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   75\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   76\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   77\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   78\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   79\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   80\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   81\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   82\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   83\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   84\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   85\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   86\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   87\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   88\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   89\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   90\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   91\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   92\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   93\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   94\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   95\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   96\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   97\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   98\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:   99\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  100\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  101\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  102\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  103\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  104\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  105\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  106\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  107\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  108\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  109\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  110\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  111\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  112\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  113\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  114\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  115\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  116\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  117\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  118\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  119\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  120\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  121\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  122\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  123\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  124\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  125\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  126\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  127\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  128\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  129\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  130\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  131\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  132\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  133\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  134\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  135\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  136\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  137\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  138\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  139\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  140\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  141\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  142\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  143\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  144\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  145\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  146\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  147\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  148\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  149\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  150\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  151\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  152\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  153\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  154\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  155\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  156\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  157\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  158\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  159\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  160\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  161\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  162\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  163\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  164\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  165\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  166\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  167\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  168\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  169\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  170\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  171\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  172\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  173\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  174\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  175\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  176\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  177\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  178\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  179\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  180\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  181\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  182\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  183\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  184\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  185\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  186\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  187\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  188\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  189\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  190\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  191\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  192\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  193\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  194\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  195\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  196\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  197\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  198\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  199\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  200\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  201\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  202\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  203\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  204\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  205\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  206\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  207\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  208\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  209\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  210\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  211\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  212\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  213\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  214\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  215\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  216\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  217\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  218\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  219\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  220\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  221\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  222\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  223\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  224\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  225\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  226\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  227\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  228\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  229\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  230\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  231\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  232\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  233\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  234\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  235\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  236\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  237\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  238\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  239\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  240\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  241\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  242\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  243\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  244\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  245\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  246\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  247\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  248\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  249\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n",
      "Epoch:  250\n",
      "\tTrain Loss: 199.69\tVal Loss: 201.65\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs/loss_with_(16, 4, 2) 2048, 0.1, 0.8, Adam, Sigmoid, L2Loss, 250, 199.69340270145713, 201.6522922608246.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m collect()\n\u001b[1;32m     39\u001b[0m name_str \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mhidden_sizes\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmomentum\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mOptimizer\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmy_activations[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mloss_layer\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mloss[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalloss[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 40\u001b[0m np\u001b[39m.\u001b[39;49msave(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mruns/loss_with_\u001b[39;49m\u001b[39m{\u001b[39;49;00mname_str\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, loss)\n\u001b[1;32m     41\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mruns/valloss_with_\u001b[39m\u001b[39m{\u001b[39;00mname_str\u001b[39m}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m, valloss)\n\u001b[1;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mruns/train_hist.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:518\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m file\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    517\u001b[0m         file \u001b[39m=\u001b[39m file \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m     file_ctx \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(file, \u001b[39m\"\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m file_ctx \u001b[39mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/loss_with_(16, 4, 2) 2048, 0.1, 0.8, Adam, Sigmoid, L2Loss, 250, 199.69340270145713, 201.6522922608246.npy'"
     ]
    }
   ],
   "source": [
    "from NeuralNetwork import NeuralNetwork as nn\n",
    "from data_layer import Dataset\n",
    "from pyflow import Tensor, L1Loss, L2Loss, FullyConnected, SGD, CrossEntropyLoss, ReLU, Sigmoid\n",
    "from gc import collect\n",
    "collect()\n",
    "\n",
    "y_val = Tensor(y_val[:, None, None]) if type(y_val) == np.ndarray else y_val\n",
    "batch_size = 2048\n",
    "dataset = Dataset(X, y, batch_size, True)\n",
    "loss_layer = L2Loss()\n",
    "net = nn(dataset, loss_layer)\n",
    "\n",
    "in_size = n_features\n",
    "hidden_sizes = (16, 4, 2)\n",
    "out_size = 1\n",
    "\n",
    "momentum = 0.8\n",
    "lr = 1e-1\n",
    "Optimizer = 'Adam'\n",
    "\n",
    "my_layers = []\n",
    "my_activations = []\n",
    "my_layers.append(FullyConnected(in_size, hidden_sizes[0], lr, Optimizer, 0.9, 0.999))\n",
    "my_layers.append(FullyConnected(hidden_sizes[0], hidden_sizes[1], lr, Optimizer, 0.9, 0.999))\n",
    "my_layers.append(FullyConnected(hidden_sizes[1], hidden_sizes[2], lr, Optimizer, 0.9, 0.999))\n",
    "my_layers.append(FullyConnected(hidden_sizes[2], out_size, lr, Optimizer, 0.9, 0.999))\n",
    "\n",
    "my_activations.append(Sigmoid())\n",
    "my_activations.append(Sigmoid())\n",
    "my_activations.append(Sigmoid())\n",
    "\n",
    "for i in range(len(my_layers)):\n",
    "    net.append_layer(my_layers[i])\n",
    "    if i < len(my_activations):\n",
    "        net.append_layer(my_activations[i])\n",
    "epochs = 250\n",
    "loss, valloss = net.train(epochs, cross_val = True, valset = (X_val, y_val), verbose = True)\n",
    "collect()\n",
    "name_str = f'{hidden_sizes} {batch_size}, {lr}, {momentum}, {Optimizer}, {my_activations[0].__class__.__name__}, {loss_layer.__class__.__name__}, {epochs}, {loss[-1]}, {valloss[-1]}'\n",
    "np.save(f'runs/loss_with_{name_str}.npy', loss)\n",
    "np.save(f'runs/valloss_with_{name_str}.npy', valloss)\n",
    "with open('runs/train_hist.txt', 'a') as f:\n",
    "    print(name_str, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_loss(net, X_val, y_val), calculate_loss(net, X, y)\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.arange(0, epochs)\n",
    "loss = net.loss\n",
    "valloss = net.vallosses\n",
    "plt.plot(t, loss, label = 'train loss')\n",
    "plt.plot(t, valloss, label = 'validation loss')\n",
    "plt.legend()\n",
    "print(valloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_model('model_Adam_48_valllos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "for i in listdir('runs'):\n",
    "    if i.startswith('loss_with_'):\n",
    "        name = i[10:]\n",
    "        train_loss = np.load(f'runs/{i}', allow_pickle=True)\n",
    "        val_loss = np.load(f'runs/valloss_with_{name}', allow_pickle=True)\n",
    "        labels = name[:-4]\n",
    "        plot(train_loss, val_loss)\n",
    "        splitted = labels.split(', ')\n",
    "        try:\n",
    "            splitted[-2] = float(splitted[-2])\n",
    "            splitted[-1] = float(splitted[-1])\n",
    "            print(\", \".join(splitted[:-2]), \"%.2f\"%splitted[-2], \"%.2f\"%splitted[-1])\n",
    "        except:\n",
    "            print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import NeuralNetwork as nn\n",
    "from data_layer import Dataset\n",
    "from pyflow import Tensor, L1Loss, L2Loss, FullyConnected, SGD, CrossEntropyLoss, ReLU\n",
    "from gc import collect\n",
    "import numpy as np\n",
    "collect()\n",
    "\n",
    "layer = FullyConnected(31, 16, 1e-3, 'SGD')\n",
    "layer2 = FullyConnected(16, 8, 1e-3, 'SGD')\n",
    "layer3 = FullyConnected(8, 4, 1e-3, 'SGD')\n",
    "layer4 = FullyConnected(4, 1, 1e-3, 'SGD')\n",
    "\n",
    "net = nn('SGD', path='saved_models/model_Adam_48_valllos')\n",
    "net.append_layer(layer)\n",
    "net.append_layer(ReLU())\n",
    "net.append_layer(layer2)\n",
    "net.append_layer(ReLU())\n",
    "net.append_layer(layer3)\n",
    "net.append_layer(ReLU())\n",
    "net.append_layer(layer4)\n",
    "\n",
    "net.loss_layer = L1Loss()\n",
    "\n",
    "net.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn('SGD')\n",
    "net2.load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFullyConnected\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m     W1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     W2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net2\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     b1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     b2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(net2\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mtolist())[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net2' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(net.layers)):\n",
    "    if net.layers[i].__class__.__name__ == 'FullyConnected':\n",
    "        W1 = np.array(net.layers[i].weights.tolist())[0]\n",
    "        W2 = np.array(net2.layers[i].weights.tolist())[0]\n",
    "        b1 = np.array(net.layers[i].bias.tolist())[0]\n",
    "        b2 = np.array(net2.layers[i].bias.tolist())[0]\n",
    "        print(np.allclose(W1, W2))\n",
    "        print(np.allclose(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.592845 0.844266 0.857946 0.847252 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pkl.dump(df_new.to_numpy(), f)\n",
    "\n",
    "with open('labels.pkl', 'wb') as f:\n",
    "    pkl.dump(labels, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
